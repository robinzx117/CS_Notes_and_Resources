https://seeing-theory.brown.edu/
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Statistics: A methodological subject encompassing all aspects of learning from data.
            Tools and methods for working with and understanding data.

Different Perspectives:
"art of summarizing data"
"science of uncertainty"
"science of decisions"
"science of variation"
"art of forecasting"
"science of measurement"
"basis for principled data collection"
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Data: 1.Numbers 2.Images 3.Words 4.Audio

Two key types of data:
    Organic/Process Data
    "Designed" Data Collection

For analyzing data, regardless of source, an important question:
Q: Can the Data be considered i.i.d.? i = independent and id = identically distributed

Variable Types:
1.Quantitative Variables: Numerical, measurable quantities in which arithmetic operations often make sense.
  Continuous (Float(float)) – could take on any value within an interval, many possible values.
  Discrete   (Integer(int)) – countable value, finite number of values.
2.Categorical (or Qualitative) Variables: Classifies individuals or items into different groups.
  Ordinal – groups have an order or ranking.
            *Only defined by how you use the data
            *Often important when creating visuals
            *Lists can hold ordinal information because they have indices
  Nominal – groups are merely names, no ranking.
            (Boolean (bool))
            (String (str))
            (None (NoneType))
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
"Study design" encompasses everything in preparation for data-driven research process.
Types of Research Studies:
-Exploratory versus Confirmatory studies
-Comparative versus Non-Comparative studies
-Observational studies versus Experiments
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-Text/CSV is a better choice than spreadsheet formats (e.g. .xlsx) for data exchange and archiving.
-Database software and tools (e.g. SQL) can be very useful for large-scale data management. Some statistical software can read data directly from a database. Another approach
 is to construct a text data file from a database e.g. using SQL.
-HDF5, Apache Parquet, and Apache Arrow are open-source standards for large binary datasets. Using these formats saves processing time relative to text/csv because fewer
 conversions are performed when reading and writing the data.
-Hadoop and Spark are two popular tools for manipulating very large datasets.

"Repeated measures" arise when multiple measurements are made on each subject in a study.
Two common formats for repeated measures data are:
-Wide format: one row per subject
-Long format: one row per measurement
Python/Pandas has tools to convert between wide and long form.

Wide Form Data
The wide form data below could come from a longitudinal study of BMI (Body Mass Index) in which BMI at ages 25, 30, and 40 are obtained for each subject.

ID          Birth_state BMI_25      BMI_30      BMI_40
1           OK          26          26          27
2           MI          23          22          28
3           FL          21          28          25

Long Form Data
The same data in long format would appear as follows:

ID          Birth_state Age         BMI
1           OK          25          26
1           OK          30          26
1           OK          40          27
2           MI          25          23
2           MI          30          22
2           MI          40          28
3           FL          25          21
3           FL          30          28
3           FL          40          25

Uses of wide and long format data layouts:
-Wide format is slightly more convenient for data entry in studies where each subject is assessed the same number of times.
-Long format is more flexible, as it accommodates measures obtained at arbitrary time points; the long format is also more natural for many forms of statistical analysis,
 such as regression analysis.
Other specialized data layouts exist for other types of data, e.g. data defined as graphs (networks of nodes and edges), images, geospatial data, or text data.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Categorical Data
-Frequency Tables: Great for numerical summaries-1.Counts 2.Percentages.
-Bar Charts:       Great for visualization.
-Pie Charts:       Use with caution.

Quantitative Data
-Histograms:
 *Shape    - Overall appearance of histogram. Can be symmetric, bell-shaped, left skewed, right skewed, etc.
 *Center   - Mean or Median.
 *Spread   - How far our data spreads. Range, Interquartile Range (IQR), standard deviation, variance.
 *Outliers - Data points that fall far from the bulk of the data.

 Normal distribution - "Empirical Rule": 68-95-99.7
 Standard Score = (Observation - Mean(µ)) / Standard Deviation(σ)

-Boxplots: provides us a very nice graphical picture of our five-number summary.
 Min Q1 Median Q3 Max
     <---IQR---->
 <------Range------->
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Multivariate Categorical Data:
1.Two-way or Contingency Tables.
2.Marginal and Conditional Distributions.
3.Graphs: Bar Charts, Side-by-side Bar Charts, Stacked Bar Charts, and Mosaic Plots.

Multivariate Quantitative Data:
Scatterplot:
*Association-Type:
 -Linear association:    the pattern is a line.
 -Quadratic association: the pattern is parabolic.
 -No association:        there is no pattern.
*Association-Direction:
 -Positive linear association.
 -Negative linear association.
*Association-Strength:
 -Weak linear association:     points are largely scattered along a line.
 -Moderate linear association: points are partially scattered along a line.
 -Strong linear association:   points are minimally scattered along a line.

Correlation
Pearson correlation (R or ⍴): number between -1 and 1 indicating the strength and sign of association between 2 variables. The sign of the correlation is the sign of the
association. The closer the number is to 1 or -1, the stronger the association.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
辛普森悖论（Simpson's Paradox）亦有人译为辛普森诡论，为英国统计学家E.H.辛普森（E.H.Simpson）于1951年提出的悖论，即在某个条件下的两组数据，分别讨论时都会满足某种性质，可是一旦合并考虑，
却可能导致相反的结论。
Confounding Variable: is an outside influence that changes the relationship between the independent and the dependent variable. It oftentimes works by affecting the causal
relationship between the primary independent variable and the dependent variable. This confounding variable confuses the relationship between two other variables; it may act
by hiding, obscuring, or enhancing the existing relationship.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Well-defined target population:
*Option 1: Conduct a Census.
*Option 2: Select a Scientific Probability Sample from the population.
 –Construct list of all units in population = sampling frame.
 –Determine probability of selection for every unit on list (known and non-zero).
 –Select units from list at random, with sampling rates for different subgroups determined by probabilities of selection.
 –Attempt to measure randomly selected units.
*Option 3: Select a Non-Probability Sample from the population.
 -Main Problems: No statistical basis for making inference about the target population; high potential for bias.

Why Probability Sampling?
-The known probabilities of selection for all units allow us to make unbiased statements about both population features and the uncertainty in survey estimates.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Simple Random Sampling (SRS)
-Start with known list of N population units, and randomly select n units from the list.
-Every unit has equal probability of selection = n / N.
-All possible samples of size n are equally likely.
-Estimates of means, proportions, and totals based on SRS are unbiased (equal to the population values on average).
-Can be with replacement or without replacement. For both: probability of selection for each unit still n / N.
-SRS rarely used in practice: collecting data from n randomly sampled units in large population can be expensive.
-All randomly sampled units will yield observations that are independent (not correlated with each other) and identically distributed (representative, in theory).
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Complex samples have certain key features:
-Population divided into different strata, and part of sample is allocated to each stratum; ensures sample representation from each stratum, and reduces variance of survey
 estimates (stratification).
–Clusters of population units (e.g., counties) are randomly sampled first (with known probability) within strata, to save costs of data collection (collect data from cases
 close to each other geographically).
–Units randomly sampled from within clusters, according to some probability of selection, and measured.

Finding a unit’s probability of selection:
-Select a out of A clusters at random in a given stratum. Then select b out of B units at random from within a selected cluster.
-Probability of selection: (a/A)*(b/B).

The inverse of a person’s probability of selection is then their sampling weight.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Features of Non-probability samples:
–Probabilities of selection can’t be determined for sampled units.
–No random selection of individual units.
–Sample divided into groups (strata) or clusters, but clusters not randomly sampled in earlier stage.
–Data collection often very cheap relative to probability sampling.

*Non-probability sample: no statistical basis for making inference about larger population from which sample selected.
*Knowing probabilities of selection (in addition to population strata and randomly sampled clusters) can estimate features of sampling distribution if were to take many random
 samples using same design.
*Sampled units not selected at random: strong risk of sampling bias (e.g., people actually interested in visiting particular web site).
*Sampled units not generally representative of larger target population of interest.
*“Big data” (e.g., information from millions of tweets) often from non-probability samples ~ be careful!

Two possible approaches:
1.Pseudo-Randomization Approach
–Combine non-probability sample with a probability sample.
–Estimate probability of being included in non-probability sample as a function of auxiliary information available in both samples.
–Treat estimated probabilities of selection as “known” for non-probability sample, use probability sampling methods for analysis.
2.Calibration Approach
–Compute weights for responding units in non-probability sample that allow weighted sampled to mirror a known population.
–Limitation: if weighting factor not related to variable(s) of interest will not reduce possible sampling bias.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Sampling distribution: distribution of survey estimates we would see if we selected many random samples using same sampling design, and computed an estimate from each.
Key properties of sampling distributions:
–Hypothetical. What would happen if we had luxury of drawing thousands of probability samples and measuring each of them?
–Generally very different in appearance from distribution of values on a single variable of interest.
–With large enough probability sample size, sampling distribution of estimates will look like a normal distribution, regardless of what estimates are being computed.
 Central Limit Theorem: CLT

Sampling variance: variability in the estimates described by the sampling distribution.
-Across hypothetical repeated samples, these sampling errors will randomly vary (some positive, some negative).
-Variability of these sampling errors describes the variance of the sampling distribution.
-If every sample estimate was equal to population quantity of interest (e.g., in the case of a Census), there would be no sampling error, and no sampling variance.
-With a larger probability sample size, sampling more from a given population: in theory there will be less sampling error, and sampling errors will be less variable.
-Larger samples, less sampling variance. More precise estimates, more confidence in inferential statements (but more costly).
-Spread of sampling distribution becomes smaller as sample size become larger.

Sampling theory allows us to estimate features of sampling distribution (including variance) based on one sample.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Given large enough samples, sampling distributions of most statistics of interest tend to normality(regardless of how the input variables are distributed).
This (Central Limit Theorem) result drives design-based statistical inference, or frequentist inference.

Properties of sampling distributions for many popular statistics (regardless of complexity):
–Normal, symmetrical, and centered at the true value.
–Larger sample sizes: less variability in estimates.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
General approaches to making population inferences based on estimated features of sampling distributions:
1.Confidence Interval Estimate for Parameters of Interest.
2.Hypothesis Testing about Parameters of Interest.
These approaches assume that sampling distributions for the estimate are (approximately) normal, which is often met if sample sizes are “large”.

-Step 1: Compute the Point Estimate: Compute an unbiased point estimate of the parameter of interest.
         Unbiased Point Estimate: average of all possible values for point estimate (a.k.a. expected value of the point estimate) is equal to true parameter value.
         Key Idea: want estimate to be unbiased with respect to sample design. If cases had unequal probabilities of selection, those weights need to be used when computing
         the point estimate.
-Step 2: Estimate the Sampling Variance of the Point Estimate: Compute an unbiased estimate of the variance of the sampling distribution for the particular point estimate.
         Unbiased Variance Estimate: Correctly describes variance of the sampling distribution under the sample design used.
         Square root of variance = Standard Error of the Point Estimate

To Form a Confidence Interval:
Best Estimate ± Margin of Error
Best Estimate = Unbiased Point Estimate
Margin of Error = “a few” Estimated Standard Errors
“a few” = multiplier from appropriate distribution based on desired confidence level and sample design
95% Confidence Level 0.05 Significance
Key Idea: 95% confidence level: expect 95% of intervals will cover true population value (if computed in this way in repeated samples).

Caution: important to get all 3 pieces right for correct inference:
1.If best estimate is not unbiased point estimate,
2.If margin of error does not use correct multiplier,
3.Does not use unbiased estimate of the standard error,
confidence interval will not have the advertised coverage.
Key Idea: Interval = range of reasonable values for parameter.
If hypothesized value for parameter lies outside confidence interval, we don’t have evidence to support that value at corresponding significance level.

Hypothesis: Could the value of the parameter be hypothesized or ‘null’ value?
Is point estimate for parameter close to this null value or far away?
Use standard error of point estimate as yardstick:
    Test statistic = (estimate - null value)/standard error
If the null is true, what is the probability of seeing a test statistic this extreme (or more extreme)? If probability small, reject the null.

These inferential procedures are valid if probability sampling was used.
What if data from a non-probability sample?
Inference approaches generally rely on modeling and combinations of data with other probability samples.
